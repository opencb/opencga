---
logLevel: "INFO"
logDir: ${OPENCGA.INSTALLATION.DIR}/logs

databasePrefix: ${OPENCGA.DB.PREFIX}
workspace: ${OPENCGA.USER.WORKSPACE}
jobDir: ${OPENCGA.USER.WORKSPACE}/jobs

panel:
  host: "http://resources.opencb.org/opencb/opencga/disease-panels"

## Configuration for Catalog databases
catalog:
  database: # MongoDB database credentials
    hosts:
      - ${OPENCGA.CATALOG.DB.HOSTS}
    user: ${OPENCGA.CATALOG.DB.USER}
    password: ${OPENCGA.CATALOG.DB.PASSWORD}
    options:
      authenticationDatabase: ${OPENCGA.CATALOG.DB.AUTHENTICATION_DATABASE}
      connectionsPerHost: ${OPENCGA.CATALOG.DB.CONNECTIONS_PER_HOST}
  ## Solr Search engine configuration, by default is the same than storage
  searchEngine:
    # List of hosts pointing either to the Solr nodes directly using a complete URL or to the zookeper nodes with HOST:PORT
    #    Example for Solr connection:       http://opencga-solr-01.zone:8983/solr
    #    Example for Zookeeper connection:  opencga-zookeeper-01:2181               <-- Recommended for replicated installations
    hosts:
      - ${OPENCGA.CATALOG.SEARCH.HOST}
    user: ""
    password: ""
    options:
      mode: "cloud"
      timeout: ${OPENCGA.CATALOG.SEARCH.TIMEOUT}
      insertBatchSize: ${OPENCGA.CATALOG.SEARCH.BATCH}

## We support multiple Authentication providers, if none is provided then we use an internal authentication implementation
authentication:
  # Session expiration time in seconds
  expiration: 3600
  authenticationOrigins:
# Custom LDAP | LDAPS configuration example
#  - id: ldaps                                              # Any id
#    type: LDAP
#    host: ldaps://localhost:636                            # or ldap://localhost:123
#    options:
#      authUserId: ""                                       # Auth user id and password if querying the LDAP system requires authentication. By default, empty.
#      authPassword: ""                                     # Auth user id and password if querying the LDAP system requires authentication. By default, empty.
#      usersSearch: dc=ge,dc=co,dc=uk                       # Mandatory field. Base search to look for users. By default, empty.
#      groupsSearch: ou=general,ou=groups,dc=ge,dc=co,dc=uk # Mandatory field. Base search to look for groups. By default, empty.
#      fullNameKey: displayname                             # Key to get the user's full name when importing users. By default, 'displayname'.
#      memberKey: member                                    # Key within groups to extract the user id. By default, 'member'.
#      dnKey: dn                                            # Key to get the user's DN or RDN unique identifier. By default, 'dn'.
#      dnFormat: "%s"                                       # Formatter to extract the user's DN if it is contained within a larger string. By default, '%s' to take the whole string.
#      uidKey: uid                                          # Key to get the user id to be used in OpenCGA. By default, 'uid'.
#      uidFormat: "%s"                                      # Formatter to extract the user id if the uid is contained within a larger string. By default, '%s' to take the whole string.
#      sslInvalidCertificatesAllowed: false                 # If using LDAPs with a custom certificate, set to true to accept any certificate. By default, false.
#      connectionTimeout: 500                               # Connection timeout value. Default: 500 ms
#      readTimeout: 1000                                    # Read timeout value. Default: 1000 ms

# Azure AD configuration example
#  - id: aad                                               # Any id
#    type: AzureAD
#    host:
#    options:
#      tenantId: xxxx              # Mandatory. Tenant id
#      authClientId: xxxx          # Mandatory. Client id of the client with permissions to authenticate users.
#      syncClientId: xxxx          # Mandatory. Client id of the client with permissions to inspect active directory.
#      syncSecretKey: xxxx         # Mandatory: Secret key of the client with permissions to inspect active directory.
#      filters: tokenField1=aa,bb,cc;tokenField2=aa,bb,cc  # Optional. Filters to be applied. OpenCGA will check if tokenField1 = aa or bb
#                # or cc and tokenField2 = aa or bb or cc. If any of the filters don't succeed, even if the user is properly authenticated
#                # in AAD, the user will not be able to generate a token and login in OpenCGA.

server:
  rest:
    port: ${OPENCGA.SERVER.REST.PORT}
    logFile: null
    defaultLimit: 2000
    maxLimit: 5000
  grpc:
    port: ${OPENCGA.SERVER.GRPC.PORT}
    logFile: null

optimizations:
  simplifyPermissions: ${OPENCGA_OPTIMIZATIONS_SIMPLIFY_PERMISSIONS}

audit:
  manager: ""             # Java manager of the audit implementation to be used to audit. If empty, catalog database will be used.
  maxDocuments: 20000000  # Maximum number of documents that will be created in the audit collection.
  maxSize: 100            # Maximum size that the audit collection will have in Gigabytes (GB).

monitor:
  daysToRemove: 30
  executionDaemonInterval: 4000 # number of milliseconds between checks
  fileDaemonInterval: 8000      # number of milliseconds between checks
  port: ${OPENCGA.MONITOR.PORT}

healthCheck:
  interval: 30 # seconds to get actual healthCheck than cache

#execution:
#  mode: ${OPENCGA.EXECUTION.MODE}
#  maxConcurrentIndexJobs : 1 # only applies to local executor
#  defaultQueue: ""
#  availableQueues: ""
#  toolsPerQueue: {}
#  options:


analysis:
  scratchDir: "${OPENCGA.ANALYSIS.SCRATCH.DIR}"    # Scratch folder for the analysis.
  execution:
    # Accepted values are "local", "SGE", "azure-batch", "k8s"
    # see org.opencb.opencga.master.monitor.executors.ExecutorFactory
    id: "${OPENCGA.EXECUTION.MODE}"
    defaultQueue: ""            # Default queue to be used to submit jobs
    availableQueues:            # Other queues for specific applications
    toolsPerQueue:
    #       docker:
    #          - "circos"
    #          - "deeptools"
    #          - "bwa"
    #          - "fastqc"
    #          - "gatk"
    #          - "picard"
    #          - "plink"
    #          - "rvtests"
    #          - "samtools"
    #          - "relatedness"
    #          - "family-qc"
    #          - "sample-qc"
    #          - "individual-qc"
    #          - "mutational-signature"
    #       fast:
    #          - "alignmentIndex"
    #       slow:
    #          - "coverage"
    #          - "alignmentStats"
    maxConcurrentJobs:
      variant-index: 20
      variant-annotation-index: 5
      variant-secondary-index: 2
    options:
      ## Job reuse policy. Do not create a new job if an equivalent PENDING or QUEUED job exists.
      jobs.reuse.enabled: true
      ## List of tools to apply the reuse policy. Add `.*` to apply widely.
      jobs.reuse.tools:
        - "variant-index"
        - "variant-stats-index"
        - "variant-annotation-index"
        - "variant-secondary-annotation-index"
      ## Local executor configuration
      local.maxConcurrentJobs: 2    # Max number of concurrent jobs to be executed locally in the master
      ## Azure Batch Service configuration example
      # azure.batchAccount : "batchAccount"
      # azure.batchKey : "batchKey"
      # azure.batchUri : "https://batchservice.uksouth.batch.azure.com"
      # azure.batchPoolId : "poolId"
      # azure.dockerImageName : "openCGADockerImageName"
      # azure.dockerArgs : "dockerRunOptions"

      ## Kubernetes executor configuration example
      # k8s.masterUrl: "https://192.168.99.100:8443/"
      k8s.clientTimeout: 30000 # ms
      k8s.imageName: "opencb/opencga-base:${project.parent.version}-hdp3.1"
      k8s.imagePullPolicy: "IfNotPresent"
      #      k8s.imagePullSecrets:
      #        name : "dockerhub-secrets-name"
      k8s.ttlSecondsAfterFinished: 3600 # s
      k8s.namespace: "default"
      # FOR Cluster Autoscaler and dedicated jobs agent pool:
      k8s.requests:
        cpu: 3
        memory: "12G"
      k8s.limits:
        cpu: 4
        memory: "13G"
      # Job java heap. If undefined, will use "requests.memory - 300MiB"
      k8s.javaHeap: "11700M"
#      k8s.envs:
#        KEY: value
      k8s.nodeSelector:
        agentpool: jobs
        kubernetes.io/os: linux
        kubernetes.io/role: agent
      # FOR ACI:
      # k8s.requests:
      #   cpu: 2
      #   memory: "12G"
      # k8s.limits:
      #   cpu: 2
      #   memory: "12G"
      # k8s.nodeSelector:
      #   kubernetes.io/role: agent
      #   kubernetes.io/os: linux
      #   type: virtual-kubelet
      # k8s.tolerations:
      # - key: virtual-kubelet.io/provider
      #   operator: Exists
      # - key: azure.com/aci
      #   effect: NoSchedule

      k8s.volumeMounts:
        - name: conf
          mountPath: /opt/opencga/conf
        - name: sessions
          mountPath: /opt/opencga/sessions
        - name: variants
          mountPath: /opt/opencga/variants
        - name: analysisconf
          mountPath: /opt/opencga/analysis
      k8s.volumes:
        - name: conf
          persistentVolumeClaim:
            claimName: "pvc-opencga-conf"
        - name: sessions
          persistentVolumeClaim:
            claimName: "pvc-opencga-sessions"
        - name: variants
          persistentVolumeClaim:
            claimName: "pvc-opencga-variants"
        - name: analysisconf
          persistentVolumeClaim:
            claimName: "pvc-opencga-analysisconf"
      k8s.podSecurityContext:
        runAsNonRoot: true
      k8s.securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        readOnlyRootFilesystem: false

  # List of analysis frameworks
  frameworks:
    - id: "local"
      available: true
      options: { }

    - id: "spark"
      available: false
      queue: "hadoop_queue" # Special executor queue to be used for jobs using this framework.
      options: # Spark properties from https://spark.apache.org/docs/latest/configuration.html#viewing-spark-properties
        spark.executor.memory: "1g"

    - id: "mapreduce"
      available: false
      queue: "hadoop_queue" # Special executor queue to be used for jobs using this framework. This is NOT a yarn queue.
      options: # MapReduce configuration from https://hadoop.apache.org/docs/r2.7.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml
        mapreduce.job.queuename: default

email:
  host: ${OPENCGA.MAIL.HOST}
  port: ${OPENCGA.MAIL.PORT}
  user: ${OPENCGA.MAIL.USER}
  password: ${OPENCGA.MAIL.PASSWORD}
  from: ${OPENCGA.MAIL.USER}
  ssl: false

#hooks:
#  user@project:study:              # Full Qualified Name of the study.
#    file:                          # Entity where the hook will be checked
#     - field: "name"               # Field of the entity to be checked
#       value: "~(.*)SV.vcf.gz$"    # Value that needs to be satisfied to perform the hook action
#       stage: "CREATE"             # Stage when the hook will be checked
#       action: "ADD"               # Action to be performed
#       where: "tags"               # Field over which the action will be performed
#       what: "SV"                  # Value to be updated
